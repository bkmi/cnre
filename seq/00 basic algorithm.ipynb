{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeed1bae-214d-4a71-a854-0df021fe6661",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7226ddd9-c5b9-43c5-8b83-ec05ecbecf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial, cache\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "from typing import Callable, Optional\n",
    "\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import numpy as np\n",
    "\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import scipy.integrate\n",
    "\n",
    "import torch\n",
    "import torch.distributions\n",
    "\n",
    "import pyro\n",
    "\n",
    "import sbi\n",
    "import sbi.analysis as analysis\n",
    "from sbi.inference import SNRE_A, SNRE_B, prepare_for_sbi, simulate_for_sbi, infer\n",
    "from sbi.utils.get_nn_models import classifier_nn\n",
    "from sbi.inference.posteriors import MCMCPosterior\n",
    "from sbi.inference.potentials.likelihood_based_potential import LikelihoodBasedPotential\n",
    "from sbi.inference.potentials.ratio_based_potential import ratio_estimator_based_potential\n",
    "\n",
    "import sbibm\n",
    "from sbibm.algorithms.sbi.utils import wrap_posterior\n",
    "\n",
    "import cnre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72e0059-611d-4cb5-81f2-7dca19cb9697",
   "metadata": {},
   "source": [
    "## generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba70ca0b-d073-4eef-9d84-65d239870158",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/sci/cnre/remote/sbibm/sbibm/tasks/slcp/task.py:91: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matricesor `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646756402876/work/aten/src/ATen/native/TensorShape.cpp:2318.)\n",
      "  m = torch.stack(\n"
     ]
    }
   ],
   "source": [
    "task = sbibm.get_task(\"slcp\")\n",
    "prior = task.get_prior_dist()\n",
    "simulator = task.get_simulator()\n",
    "x_o = task.get_observation(1)\n",
    "theta_o = task.get_true_parameters(1)\n",
    "transform = task._get_transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22c4c189-55ff-4c89-ac5c-33e59c83e50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClosedFormPotential(sbi.inference.potentials.base_potential.BasePotential):\n",
    "    def __init__(\n",
    "        self, \n",
    "        fn: Callable,\n",
    "        prior: torch.distributions.Distribution, \n",
    "        x_o: Optional[torch.Tensor] = None, \n",
    "        device: str = \"cpu\"\n",
    "    ):\n",
    "        super().__init__(prior, x_o, device)\n",
    "        self.fn = fn\n",
    "    \n",
    "    def __call__(self, theta: torch.Tensor, track_gradients: bool = True) -> torch.Tensor:\n",
    "        with torch.set_grad_enabled(track_gradients):\n",
    "            return self.fn(theta)\n",
    "    \n",
    "    @property\n",
    "    def allow_iid_x(self) -> bool:\n",
    "        raise NotImplementedError\n",
    "\n",
    "def create_chain(\n",
    "    starting_theta: torch.Tensor, \n",
    "    potential_fn: Callable, \n",
    "    transform: Optional = None,\n",
    "    thin: int = 10,\n",
    "    num_workers: int = 1,\n",
    "    device: Optional = None,\n",
    "    x_shape: Optional = None, \n",
    "):\n",
    "    \"\"\"starting_theta [num_starting_pts, theta_dim] - we try to make this an array of samples drawn from the prior which then start each chain.\"\"\"\n",
    "    if transform is not None:\n",
    "        starting_theta = transform(starting_theta)\n",
    "    # delta = pyro.distributions.delta.Delta(starting_theta)\n",
    "    posterior = MCMCPosterior(\n",
    "        potential_fn=potential_fn,\n",
    "        # proposal=starting_theta,  # should be delta, but the support of the prior has been transformed to the reals (so should the point)\n",
    "        proposal=prior,  # should be delta, but the support of the prior has been transformed to the reals (so should the point)\n",
    "        theta_transform=torch.distributions.transforms.identity_transform,  # this is the identity when running sbibm\n",
    "        method=\"slice_np_vectorized\",\n",
    "        # method=\"slice\",\n",
    "        thin=thin,\n",
    "        warmup_steps=0,\n",
    "        num_chains=starting_theta.shape[0],\n",
    "        init_strategy=\"proposal\",\n",
    "        # init_strategy=\"delta\",\n",
    "        num_workers=num_workers,\n",
    "        device=device,\n",
    "        x_shape=x_shape,\n",
    "    )\n",
    "    return wrap_posterior(posterior, transform)\n",
    "    # samples = posterior.sample((self.num_posterior_samples,), x=observation).detach()\n",
    "    # TODO figure out whether the prior and likelihood need to be transformed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cadc8563-a280-4a3b-aae6-25fc4a3b7288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7522b87a519d4421b9afe7996c5a3469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running 500 simulations.:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Neural network successfully converged after 45 epochs."
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "d = theta_o.shape[1]\n",
    "dd = x_o.shape[1]\n",
    "starting_theta = torch.rand(1, d)\n",
    "\n",
    "inference = SNRE_A(prior)\n",
    "theta, x = simulate_for_sbi(simulator, prior, num_simulations=500)\n",
    "ratio_estimator = inference.append_simulations(theta, x).train()\n",
    "\n",
    "class Ratio(torch.nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(torch.nn.Linear(10, 10))\n",
    "        self.map = torch.randn(dd, d)\n",
    "        self.net.to(device)\n",
    "    \n",
    "    def forward(self, w: list):\n",
    "        \"\"\"Inputs to (s)nre classifier must be a list containing raw theta and x.\"\"\"\n",
    "        # return -torch.sum(w[0] ** 2, dim=-1)\n",
    "        theta, x = w\n",
    "        assert isinstance(theta, torch.Tensor)\n",
    "        assert len(theta.shape) == 2\n",
    "        \n",
    "        print(self.map)\n",
    "        print(theta)\n",
    "        print(x)\n",
    "        t = torch.einsum(\"ij,bj->bi\", self.map, theta)\n",
    "        return (t - x).pow(2).neg().sum(dim=-1, keepdims=True)\n",
    "\n",
    "ratio = Ratio(\"cpu\")\n",
    "    \n",
    "potential, _ = ratio_estimator_based_potential(\n",
    "    ratio, \n",
    "    prior, \n",
    "    x_o=x_o, \n",
    "    enable_transform=False\n",
    ")\n",
    "p = create_chain(starting_theta, potential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da2a14e8-aefd-45e0-ae2d-ae3ceba86f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_built = inference.build_posterior(mcmc_method=\"slice_np_vectorized\")\n",
    "p_built = wrap_posterior(p_built, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b888d10e-f8ff-4971-aec2-92c03f2d949c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2899,  1.0014, -1.7746, -2.5819,  0.2798],\n",
      "        [ 1.4384, -0.0464, -0.0704,  0.9918,  1.0043],\n",
      "        [-2.4651, -0.6654,  0.4301, -1.8725,  0.3934],\n",
      "        [-0.3969, -2.9779,  0.3740,  1.5982,  0.4220],\n",
      "        [ 0.5613,  0.1550, -0.2588, -0.2382,  0.5420],\n",
      "        [ 1.8282,  0.4050, -1.5480, -0.6493,  1.0415],\n",
      "        [-1.8636, -0.2498,  1.8554,  0.2767, -0.3876],\n",
      "        [-0.4782,  0.6687, -0.0266, -0.2716, -0.3646]])\n",
      "tensor([[0.5864, 0.4337, 0.7282, 0.3138, 0.8866],\n",
      "        [0.6034, 0.6280, 0.4096, 0.2352, 0.1128]])\n",
      "tensor([[  2.3719,   0.4995,   9.9314,   1.7137, -10.4364,  -1.9068,  -1.2344,\n",
      "          -0.0974],\n",
      "        [  2.3719,   0.4995,   9.9314,   1.7137, -10.4364,  -1.9068,  -1.2344,\n",
      "          -0.0974]])\n",
      "tensor([-293.9955, -296.8475])\n",
      "tensor([-9.1700, -9.0978], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "aaa = torch.rand(2, d)\n",
    "p.flow.potential_fn.set_x(x_o)\n",
    "print(p.flow.potential_fn(aaa))\n",
    "\n",
    "p_built.flow.potential_fn.set_x(x_o)\n",
    "print(p_built.flow.potential_fn(aaa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdbea767-f068-4d43-a1c3-e30105f9c92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a80785a96d14bce9d350f441c8af350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running vectorized MCMC with 5 chains:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2899,  1.0014, -1.7746, -2.5819,  0.2798],\n",
      "        [ 1.4384, -0.0464, -0.0704,  0.9918,  1.0043],\n",
      "        [-2.4651, -0.6654,  0.4301, -1.8725,  0.3934],\n",
      "        [-0.3969, -2.9779,  0.3740,  1.5982,  0.4220],\n",
      "        [ 0.5613,  0.1550, -0.2588, -0.2382,  0.5420],\n",
      "        [ 1.8282,  0.4050, -1.5480, -0.6493,  1.0415],\n",
      "        [-1.8636, -0.2498,  1.8554,  0.2767, -0.3876],\n",
      "        [-0.4782,  0.6687, -0.0266, -0.2716, -0.3646]])\n",
      "tensor([[-1.3040,  1.7884, -1.2323, -2.4065, -2.3238],\n",
      "        [-2.3774, -2.8776, -0.3343, -0.4853,  2.7386],\n",
      "        [ 0.6703,  1.0233, -2.6752, -1.6807, -1.7447],\n",
      "        [ 1.9382, -0.7606, -2.2653,  1.3156, -1.2346],\n",
      "        [-2.4845, -0.2214,  2.4168,  2.6739, -2.5722]])\n",
      "tensor([[  2.3719,   0.4995,   9.9314,   1.7137, -10.4364,  -1.9068,  -1.2344,\n",
      "          -0.0974],\n",
      "        [  2.3719,   0.4995,   9.9314,   1.7137, -10.4364,  -1.9068,  -1.2344,\n",
      "          -0.0974],\n",
      "        [  2.3719,   0.4995,   9.9314,   1.7137, -10.4364,  -1.9068,  -1.2344,\n",
      "          -0.0974],\n",
      "        [  2.3719,   0.4995,   9.9314,   1.7137, -10.4364,  -1.9068,  -1.2344,\n",
      "          -0.0974],\n",
      "        [  2.3719,   0.4995,   9.9314,   1.7137, -10.4364,  -1.9068,  -1.2344,\n",
      "          -0.0974]])\n",
      "tensor([[ 0.2899,  1.0014, -1.7746, -2.5819,  0.2798],\n",
      "        [ 1.4384, -0.0464, -0.0704,  0.9918,  1.0043],\n",
      "        [-2.4651, -0.6654,  0.4301, -1.8725,  0.3934],\n",
      "        [-0.3969, -2.9779,  0.3740,  1.5982,  0.4220],\n",
      "        [ 0.5613,  0.1550, -0.2588, -0.2382,  0.5420],\n",
      "        [ 1.8282,  0.4050, -1.5480, -0.6493,  1.0415],\n",
      "        [-1.8636, -0.2498,  1.8554,  0.2767, -0.3876],\n",
      "        [-0.4782,  0.6687, -0.0266, -0.2716, -0.3646]])\n",
      "tensor([[-1.3040,  1.7841, -1.2323, -2.4065, -2.3238],\n",
      "        [-2.3774, -2.8871, -0.3343, -0.4853,  2.7386],\n",
      "        [ 0.6703,  1.0233, -2.6752, -1.6837, -1.7447],\n",
      "        [ 1.9382, -0.7606, -2.2653,  1.3156, -1.2421],\n",
      "        [-2.4867, -0.2214,  2.4168,  2.6739, -2.5722]])\n",
      "tensor([[  2.3719,   0.4995,   9.9314,   1.7137, -10.4364,  -1.9068,  -1.2344,\n",
      "          -0.0974],\n",
      "        [  2.3719,   0.4995,   9.9314,   1.7137, -10.4364,  -1.9068,  -1.2344,\n",
      "          -0.0974],\n",
      "        [  2.3719,   0.4995,   9.9314,   1.7137, -10.4364,  -1.9068,  -1.2344,\n",
      "          -0.0974],\n",
      "        [  2.3719,   0.4995,   9.9314,   1.7137, -10.4364,  -1.9068,  -1.2344,\n",
      "          -0.0974],\n",
      "        [  2.3719,   0.4995,   9.9314,   1.7137, -10.4364,  -1.9068,  -1.2344,\n",
      "          -0.0974]])\n",
      "tensor([-375.2679, -218.0058, -481.7341, -545.1812, -656.8799])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjoblib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parallel_backend\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreading\u001b[39m\u001b[38;5;124m'\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_o\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sci/cnre/remote/sbibm/sbibm/utils/nflows.py:326\u001b[0m, in \u001b[0;36mFlowWrapper.sample\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 326\u001b[0m     Y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39minv(Y)\n",
      "File \u001b[0;32m~/sci/cnre/remote/sbi/sbi/inference/posteriors/mcmc_posterior.py:244\u001b[0m, in \u001b[0;36mMCMCPosterior.sample\u001b[0;34m(self, sample_shape, x, method, thin, warmup_steps, num_chains, init_strategy, init_strategy_num_candidates, mcmc_parameters, mcmc_method, sample_with, num_workers, show_progress_bars)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(track_gradients):\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslice_np\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslice_np_vectorized\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 244\u001b[0m         transformed_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slice_np_mcmc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpotential_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpotential_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m            \u001b[49m\u001b[43minitial_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m            \u001b[49m\u001b[43mthin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    249\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[1;32m    250\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvectorized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mslice_np_vectorized\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshow_progress_bars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhmc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnuts\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslice\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    255\u001b[0m         transformed_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pyro_mcmc(\n\u001b[1;32m    256\u001b[0m             num_samples\u001b[38;5;241m=\u001b[39mnum_samples,\n\u001b[1;32m    257\u001b[0m             potential_function\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpotential_,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m             show_progress_bars\u001b[38;5;241m=\u001b[39mshow_progress_bars,\n\u001b[1;32m    264\u001b[0m         )\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/sci/cnre/remote/sbi/sbi/inference/posteriors/mcmc_posterior.py:402\u001b[0m, in \u001b[0;36mMCMCPosterior._slice_np_mcmc\u001b[0;34m(self, num_samples, potential_function, initial_params, thin, warmup_steps, vectorized, num_workers, show_progress_bars)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;124;03m\"\"\"Custom implementation of slice sampling using Numpy.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \n\u001b[1;32m    384\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03mReturns: Tensor of shape (num_samples, shape_of_single_theta).\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    400\u001b[0m num_chains, dim_samples \u001b[38;5;241m=\u001b[39m initial_params\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m--> 402\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mslice_np_parallized\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpotential_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvectorized\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvectorized\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshow_progress_bars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# Save sample as potential next init (if init_strategy == 'latest_sample').\u001b[39;00m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mcmc_init_params \u001b[38;5;241m=\u001b[39m samples[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\u001b[38;5;241m.\u001b[39mreshape(num_chains, dim_samples)\n",
      "File \u001b[0;32m~/sci/cnre/remote/sbi/sbi/samplers/mcmc/slice_numpy.py:527\u001b[0m, in \u001b[0;36mslice_np_parallized\u001b[0;34m(potential_function, initial_params, num_samples, thin, warmup_steps, vectorized, num_workers, show_progress_bars)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;66;03m# Show progress bars over batches.\u001b[39;00m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm_joblib(\n\u001b[1;32m    519\u001b[0m     tqdm(\n\u001b[1;32m    520\u001b[0m         \u001b[38;5;28mrange\u001b[39m(num_batches),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    525\u001b[0m     )\n\u001b[1;32m    526\u001b[0m ):\n\u001b[0;32m--> 527\u001b[0m     all_samples \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_fun\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minitial_params_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minitial_params_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minitial_params_in_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m     all_samples \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(all_samples)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    532\u001b[0m     samples \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(all_samples)\n",
      "File \u001b[0;32m~/sci/cnre/env/lib/python3.9/site-packages/joblib/parallel.py:1043\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;66;03m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;66;03m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;66;03m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m     \u001b[38;5;66;03m# remaining jobs.\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[0;32m~/sci/cnre/env/lib/python3.9/site-packages/joblib/parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/sci/cnre/env/lib/python3.9/site-packages/joblib/parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[0;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[0;32m~/sci/cnre/env/lib/python3.9/site-packages/joblib/_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[1;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[0;32m~/sci/cnre/env/lib/python3.9/site-packages/joblib/_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[0;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sci/cnre/env/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/sci/cnre/env/lib/python3.9/site-packages/joblib/parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[0;32m~/sci/cnre/remote/sbi/sbi/samplers/mcmc/slice_numpy.py:503\u001b[0m, in \u001b[0;36mslice_np_parallized.<locals>.run_slice_np_vectorized\u001b[0;34m(inits, seed)\u001b[0m\n\u001b[1;32m    501\u001b[0m warmup_ \u001b[38;5;241m=\u001b[39m warmup_steps \u001b[38;5;241m*\u001b[39m thin\n\u001b[1;32m    502\u001b[0m num_samples_ \u001b[38;5;241m=\u001b[39m ceil((num_samples \u001b[38;5;241m*\u001b[39m thin) \u001b[38;5;241m/\u001b[39m num_chains)\n\u001b[0;32m--> 503\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[43mposterior_sampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwarmup_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_samples_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m samples \u001b[38;5;241m=\u001b[39m samples[:, warmup_:, :]  \u001b[38;5;66;03m# discard warmup steps\u001b[39;00m\n\u001b[1;32m    505\u001b[0m samples \u001b[38;5;241m=\u001b[39m samples[:, ::thin, :]  \u001b[38;5;66;03m# thin chains\u001b[39;00m\n",
      "File \u001b[0;32m~/sci/cnre/remote/sbi/sbi/samplers/mcmc/slice_numpy.py:323\u001b[0m, in \u001b[0;36mSliceSamplerVectorized.run\u001b[0;34m(self, num_samples)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLOWER\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28mprint\u001b[39m(log_probs[c])\n\u001b[1;32m    322\u001b[0m     outside_lower \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 323\u001b[0m         log_probs[c] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m sc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogu\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m sc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcxi\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m sc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width\n\u001b[1;32m    325\u001b[0m     )\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m outside_lower:\n\u001b[1;32m    328\u001b[0m         sc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlx\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m sc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwi\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "from joblib import parallel_backend\n",
    "with parallel_backend('threading', n_jobs=1):\n",
    "    p.sample((10,), x=x_o)\n",
    "# For some reason, this one has shape issues but the one above does not. it could be due to the distribution i'm using (but I don't think so)\n",
    "# maybe the ratio estiamtor is bad somehow.\n",
    "\n",
    "# maybe the potential funciton requires a default x and this one doesn't give it? or maybe it needs an option to give an x?\n",
    "\n",
    "# After many tests ( and the creation of this Ratio class above), I think the issue lies in the log_prob being too high dimensional.\n",
    "# I think the way they create this potential function requires some scrutiny. Especially the setting of the x_o.\n",
    "\n",
    "## p.s. don't forget you have a stashed update to the repo. (not sure if it's important.) I think it was created before you did all the merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fec7057-5db6-44a0-915f-1a7f3b511c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2d98ca-daad-4f3d-99a1-4c26d0f2cf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(prior, num_observation, observation_seed, **kwargs):\n",
    "    np.random.seed(observation_seed)\n",
    "    torch.manual_seed(observation_seed)\n",
    "\n",
    "    prior = self.get_prior()\n",
    "    true_parameters = prior(num_samples=1)\n",
    "\n",
    "    simulator = self.get_simulator()\n",
    "    observation = simulator(true_parameters)\n",
    "\n",
    "    if create_reference:\n",
    "        reference_posterior_samples = self._sample_reference_posterior(\n",
    "            num_observation=num_observation,\n",
    "            num_samples=self.num_reference_posterior_samples,\n",
    "            **kwargs,\n",
    "        )\n",
    "        num_unique = torch.unique(reference_posterior_samples, dim=0).shape[0]\n",
    "        assert num_unique == self.num_reference_posterior_samples\n",
    "        self._save_reference_posterior_samples(\n",
    "            num_observation,\n",
    "            reference_posterior_samples,\n",
    "        )\n",
    "\n",
    "Parallel(n_jobs=n_jobs, verbose=50, backend=\"loky\")(\n",
    "    delayed(run)(num_observation, observation_seed, **kwargs)\n",
    "    for num_observation, observation_seed in enumerate(\n",
    "        self.observation_seeds, start=1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a5e15f-d12b-4327-a0dc-c90175e90191",
   "metadata": {},
   "source": [
    "## setup problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40202464-7311-4fb3-8e8f-0bbac510d38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_total_samples = 2 ** 13\n",
    "validation_fraction=2 ** -2\n",
    "num_training_samples = int((1 - validation_fraction) * num_total_samples)\n",
    "num_validation_samples = int(validation_fraction * num_total_samples)\n",
    "num_posterior_samples = 3_000\n",
    "\n",
    "training_batch_size=2 ** 10\n",
    "learning_rate=3e-4\n",
    "stop_after_epochs=2 ** 31 - 1\n",
    "# max_num_epochs=1_000  # We want the network to see the same number of batches no matter how much data we provide\n",
    "num_batches_to_see = 8_000\n",
    "clip_max_norm=None\n",
    "\n",
    "classifier_kwargs = dict(\n",
    "    model='resnet', \n",
    "    hidden_features=50, \n",
    "    num_blocks=2,\n",
    "    dropout_probability=0.0,\n",
    "    use_batch_norm=True\n",
    ")\n",
    "sample_with = \"rejection\"\n",
    "\n",
    "device = \"cpu\"\n",
    "root = Path(\"figures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e6df87-299a-42be-831a-4d0611f20d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "kind = \"gaussian\"\n",
    "if kind == \"parabola\":\n",
    "    dimension = 3\n",
    "    low = -torch.ones(dimension).mul(3.).sqrt().to(device)\n",
    "    high = torch.ones(dimension).mul(3.).sqrt().to(device)\n",
    "    limits = torch.stack([low.cpu(), high.cpu()], dim=-1).numpy()\n",
    "    prior = sbi.utils.BoxUniform(low=low, high=high, device=device)\n",
    "    task = cnre.Parabola(scale=0.1)\n",
    "    simulate = task.simulate\n",
    "    simulator, prior = prepare_for_sbi(simulate, prior)\n",
    "    true_theta = torch.ones(dimension).to(device).unsqueeze(0)\n",
    "    observation = simulator(true_theta)\n",
    "elif kind == \"gaussian\":\n",
    "    dimension = 3\n",
    "    low = -torch.ones(dimension).mul(3.).sqrt().to(device)\n",
    "    high = torch.ones(dimension).mul(3.).sqrt().to(device)\n",
    "    limits = torch.stack([low.cpu(), high.cpu()], dim=-1).numpy()\n",
    "    prior = sbi.utils.BoxUniform(low=low, high=high, device=device)\n",
    "    task = cnre.Gaussian(scale=0.1)\n",
    "    simulate = task.simulate\n",
    "    simulator, prior = prepare_for_sbi(simulate, prior)\n",
    "    true_theta = torch.ones(dimension).to(device).unsqueeze(0)\n",
    "    observation = simulator(true_theta)\n",
    "elif kind == \"slcp\":\n",
    "    tt = sbibm.get_task(\"slcp\")\n",
    "    num_observation = 1\n",
    "    simulator = tt.get_simulator()\n",
    "    prior = tt.get_prior_dist()\n",
    "    dimension = tt.dim_data\n",
    "    observation = tt.get_observation(num_observation)\n",
    "    true_theta = tt.get_true_parameters(num_observation)\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "theta = prior.sample((num_total_samples,)).to(device)\n",
    "x = simulator(theta).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92e2fe8-5521-4da7-8f04-c5aa3ae889e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_classifier = classifier_nn(**classifier_kwargs)\n",
    "classifier = get_classifier(theta[:training_batch_size, ...], x[:training_batch_size, ...])\n",
    "classifier.to(device)\n",
    "\n",
    "# optimizer = torch.optim.SGD(classifier.parameters(), lr=lr, momentum=momentum)\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b64269a-e8dd-493d-8433-340418627bf5",
   "metadata": {},
   "source": [
    "## experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762c39ec-1bec-4321-b8b3-1d868a87b2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.utils.data.TensorDataset(theta, x)\n",
    "train_set, valid_set = torch.utils.data.random_split(dataset, [num_training_samples, num_validation_samples])\n",
    "train_loader = torch.utils.data.DataLoader(train_set, training_batch_size, shuffle=True, drop_last=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, training_batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90163cea-6271-48bb-b356-3f2d607f7fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_epochs = round(num_batches_to_see / len(train_loader))\n",
    "max_num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ebaab8-4f97-41e2-8a18-6be76e839fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "l1 = cnre.loss_bce(classifier, theta[:256], x[:256])\n",
    "torch.manual_seed(0)\n",
    "l2 = cnre.loss(classifier, theta[:256], x[:256], 2, gamma=1.0, reuse=True)\n",
    "\n",
    "# The issue here is that the BCE takes the mean of more terms, my version doesn't do that.\n",
    "# l2 - l1.reshape(-1, 2).sum(dim=-1) * 0.5\n",
    "# l2.mean() - l1.mean()\n",
    "l2 - l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12810f3f-3143-4565-a97d-c3bce80510b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doit(num_atoms: int):\n",
    "    gamma = 1.0\n",
    "    results = cnre.algorithms.cnre.train(\n",
    "        classifier, \n",
    "        optimizer, \n",
    "        max_num_epochs, \n",
    "        train_loader, \n",
    "        valid_loader, \n",
    "        num_atoms,\n",
    "        gamma=gamma,\n",
    "    )\n",
    "    name = f\"num atoms {num_atoms}\"\n",
    "    plt.plot(results['valid_losses'])\n",
    "    plt.title(name)\n",
    "    classifier.load_state_dict(results[\"best_network_state_dict\"])\n",
    "    posterior = cnre.get_sbi_posterior(\n",
    "        ratio_estimator=classifier,\n",
    "        prior=prior,\n",
    "        sample_with=\"rejection\", \n",
    "        mcmc_method=\"slice_np\",\n",
    "        mcmc_parameters={},\n",
    "        rejection_sampling_parameters={},\n",
    "    )\n",
    "    samples = posterior.sample((num_posterior_samples,), x=observation.cpu())\n",
    "    fig, _ = analysis.pairplot(\n",
    "        samples.cpu().numpy(), \n",
    "        figsize=(6,6), \n",
    "        points=true_theta.cpu().numpy(),\n",
    "        title=name,\n",
    "        # limits=limits,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d520bd1-bd63-4727-9be0-da96de5f4e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = f\"ref\"\n",
    "fig, _ = analysis.pairplot(\n",
    "    tt.get_reference_posterior_samples(num_observation), \n",
    "    figsize=(6,6), \n",
    "    points=true_theta.cpu().numpy(),\n",
    "    title=name,\n",
    "    # limits=limits,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1bc574-84f0-40c5-8652-40575b8e8b64",
   "metadata": {},
   "source": [
    "### gilles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5e57ca-57a4-47d7-b55d-f06c13844bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "doit(num_atoms=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce307612-a9a0-4a67-826f-b70dabf414b1",
   "metadata": {},
   "source": [
    "### ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7dad9b-6e3a-424c-9016-4f0fe96dc91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doit(num_atoms=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700c5278-d9f6-4fe7-89f1-a4e881c23b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
